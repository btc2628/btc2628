# Define the model and the pipeline
pipeline = Pipeline([
    ('scale', StandardScaler()), 
    ('model', LogisticRegression(max_iter=5000, solver='saga'))  # Using 'saga' solver for its versatility
])

# Define the parameter space for RandomizedSearch
param_distributions = {
    'model__C': loguniform(1e-4, 1e0),  # Regularization strength on a log scale
    'model__penalty': ['elasticnet'],  # Using elasticnet
    'model__l1_ratio': uniform(0, 1)  # Mixing ratio of l1 and l2 penalties in elasticnet
}

# Define the KFold cross-validator
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Setup the RandomizedSearchCV
random_search = RandomizedSearchCV(pipeline, param_distributions=param_distributions, n_iter=100, cv=kfold, verbose=1, random_state=42, n_jobs=-1)

# Fit the RandomizedSearchCV to find the best model
random_search.fit(X, Y)

# Best model after Randomized Search
print("Best parameters found: ", random_search.best_params_)
print("Best score: ", random_search.best_score_)

best_model = random_search.best_estimator_.named_steps['model']

# Get feature names directly from the DataFrame
feature_names = np.array(X.columns)  # Extracting feature names

# Get the coefficients from the best model
coefficients = best_model.coef_[0]

# Plotting
plt.figure(figsize=(10, 8))
indices = np.argsort(np.abs(coefficients))[::-1]  # Sort features by importance
plt.barh(range(len(indices)), coefficients[indices], color='b', align='center')
plt.yticks(range(len(indices)), feature_names[indices])
plt.xlabel('Coefficient Value')
plt.title('Feature Importance - Logistic Regression')
plt.gca().invert_yaxis()  # Invert y-axis to have the most important at the top
plt.show()

