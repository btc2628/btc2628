from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from scipy.stats import randint
import matplotlib.pyplot as plt
import numpy as np

# Define the pipeline
pipeline = Pipeline([
    ('scale', StandardScaler()), 
    ('model', DecisionTreeClassifier(random_state=42))
])

# Define the parameter distributions for RandomizedSearch
param_distributions = {
    'model__max_depth': randint(1, 10),  # Maximum number of levels in the tree
    'model__min_samples_split': randint(2, 20),  # Minimum number of samples required to split a node
    'model__min_samples_leaf': randint(1, 20),  # Minimum number of samples required at each leaf node
}

# Define the KFold cross-validator
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Setup the RandomizedSearchCV
random_search = RandomizedSearchCV(pipeline, param_distributions=param_distributions, n_iter=100, cv=kfold, verbose=1, random_state=42, n_jobs=-1)

# Fit the RandomizedSearchCV to find the best model
random_search.fit(X, Y)

# Assuming 'X' is a pandas DataFrame and extracting feature names
feature_names = X.columns

# Extract the best model from the fitted RandomizedSearchCV object
best_model = random_search.best_estimator_.named_steps['model']

# Get the feature importances
importances = best_model.feature_importances_

# Plotting
plt.figure(figsize=(10, 8))
indices = np.argsort(importances)[::-1]  # Sort features by importance
plt.barh(range(len(indices)), importances[indices], color='steelblue', align='center')
plt.yticks(range(len(indices)), feature_names[indices])
plt.xlabel('Importance')
plt.title('Feature Importance - Decision Tree')
plt.gca().invert_yaxis()  # Invert y-axis to have the most important at the top
plt.show()
